Consistency checks are based on the JSON response, not based on the implemented evaluation logic:
    FV != very first project version (?)
    #FV = 1
    #AV > 1
    git commit related



Is it necessary to sort versions? Could implicate problems during IV and OV evaluation?

Inconsistency between no AV (empty array) and no IV (null). Check what "missing" var means in JSON context and Model context and csv context.

Check if OV is correctly evaluated.

Sonar link plugin and SonarCloud.

x% to include/exclude.

Proportion.

Remove warning where it is possible.

Read all the codebase again.

Do we want non-released versions?

make the ticket fields real version objects?

no versions names duplicates are supported.

CsvExporter saves too much info.

according to OPERATE with versions, we must use dates. (some version "names" are inconsistent with their release dates) (in this code we are never sorting them, but they are sorted by date in the csv probably because JSON versions response are already sorted by date)
for example AV (builded on versions field of JSON) sorts by ascending date order, instead of FV that sorts by descending order. we can not delete versions with inconsistent date and names like 4.3.0 and 4.2.3 of BOOKKEEPER

what if no versions are found?

we use different null checks on different JSON fields.

we assume that all versions have a release date.

OV is null if creation date is before first release.

how OV is evaluated?

released" field never used for evaluation.

lots of ticket will be removed because FV = AV

new push is equal to new version? what if 4.0.0 is AV and a push (still 4.0.0 not 4.0.1) solved the bug and now 4.0.0 is FV ?

NOT ALL VERSION HAVE A RELEASE DATE, WE CUT THEM OFF.

clone every day because new tickets could be possibly pushed.

lots of rows have not FV but are actually status fixed. we cut them off.

what can we do with more FV ? use the last is probably ok or mabe cut them off. is possible that a ticket war reopened, so we take the last FV

What is better between "lot of data with low accuracy" and "low data with lot of accuracy" ? This question will define what to cut from the rows and what to estimate.

we will retrive commits linket to ticket , not ticket linket to commit. the process is : take the ticket id, retrive commits with that id inside the comment. so first we clean them, and after we retrive commits.

we could use proportion also when oldest AV > OP

remove comments

maybe create a component to get IV from AV , because we evaluate IV at the start and also after proportion.

we can use proportion also in some cutted off ticket...

tickets have inside versions, that have inside dates. so probably there is some redundant jira version array in the code.

Divide the logic of TicketCleaningService.java

return of csv cleaner must be a string to print on the terminal as other csv exporter.

must realize proportion.

do not use stream.

controller.cleanTickets(tickets, projectKey) should not take projectKey param

service methods should not call other methods. for example retrieveTickets() should not call getProjectVersions() inside it, but the controller should call retrieveTickets(getProjectVersions())

check url return if null or empty.

check redundant parameters.

do not take empty data. instead, we keep the default values.

add logs according to show unexpected data, like an empty version list.


warning, commented out code, refactor--> cleaning--> right logic?---> null checks --> understand all of the code--> remove all unnecessary print and logs

why jiracontroller wants his private fields?

maybe we can make jiraservice full static and not instanciate it inside the controller

ticket evaluate also versions, so avoid double execution

could create exporter controller

could use wrapper to avoid inconsistency between missing info

some missin-info should interrupt the execution of the program, others should not.

make a final check on all columns of the csv. remove those rows that have a particular column = missing info (like empty string, null, 0, empty list, or others...)             ->          so how is represented a missing info should not concern us particularly

lot of refactoring could be done, but now is not the focus because this is still the first step of the whole project. we will se with sonarcloud or alternatives.

start sonarqube

checking if json are not missing, but if "" is found it is saved. then we will normalize the csv missing info.

whitespaces as names not checked.

modify which info to retrieve.

normalize csv. tickets are not normalized reguarding missing info.

split the file logic.

migrate the parsing exception to the controller.

check all comments

now we must confirm that all info are correctly retrived -> filter out some tickets -> evaluate some missing info for some tickets.

examples of filtering:
    OV missing because of no versions before ticket opening
    multiple FV
    temporal inconsistency
    AV and IV can be evaluated with proportion (cold start and then incremental)
    (check slides and notebook)

FV are retrived using jira tickets, if multiple take the last one, if missing remove row.

use proportion, remove some tickets, 33% initial tickets.

-----------

some exceptions are caught inside jiraservice, others by controller.

too much logs.

translate to english comments.

understand better csv exporter.

clean info logs.

sonar

mvn documentation?

print a better log

check new refactored code